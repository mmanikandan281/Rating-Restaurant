# -*- coding: utf-8 -*-
"""Rating Restaurant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d7qfvbN5utqV-IzdfnmQ95_tsl8rUF28

# Task

Objective: Build a machine learning model to predict the
aggregate rating of a restaurant based on other features.

Steps:

Preprocess the dataset by handling missing values,
encoding categorical variables, and splitting the data
into training and testing sets.
Select a regression algorithm (e.g., linear regression,
decision tree regression) and train it on the training data.
Evaluate the model's performance using appropriate
regression metrics (e.g., mean squared error, R-squared)
on the testing data.
Interpret the model's results and analyze the most
influential features affecting restaurant ratings.

Here is all the data you need:
"Dataset .csv"

## Data loading

### Subtask:
Load the dataset.

**Reasoning**:
Load the dataset and display some information to verify it loaded correctly.
"""

import pandas as pd
df = pd.read_csv('/content/Dataset .csv', sep=';')
display(df.head())
print(df.shape)
print(df.dtypes)

"""## Data exploration

### Subtask:
Explore the dataset to understand its structure, identify the target variable ("aggregate rating"), and examine the distribution of features.

"""

import pandas as pd

# The data is likely comma-separated within a single column
# First, read the file assuming a single column with semicolon delimiter
df = pd.read_csv('/content/Dataset .csv', sep=';')

# Check the initial structure of the DataFrame
print("Initial DataFrame shape:", df.shape)

# Then, split the first column by commas to create individual columns
df = df[df.columns[0]].str.split(',', expand=True)

# Check the number of columns after splitting
print("After split DataFrame shape:", df.shape)

# Adjust column names based on the actual number of columns
# If there are more or fewer columns, add/remove names accordingly
column_names = [
    'Restaurant ID', 'Restaurant Name', 'Country Code', 'City', 'Address',
    'Locality', 'Locality Verbose', 'Longitude', 'Latitude', 'Cuisines',
    'Average Cost for two', 'Currency', 'Has Table booking', 'Has Online delivery',
    'Is delivering now', 'Switch to order menu', 'Price range', 'Aggregate rating',
    'Rating color', 'Rating text', 'Votes'
]

# Assign column names only if they match the number of columns
if len(column_names) == df.shape[1]:
    df.columns = column_names
else:
    print(f"Warning: Expected {len(column_names)} columns but got {df.shape[1]} columns. Check the dataset!")

# Display the first few rows of the DataFrame
display(df.head())

import pandas as pd

# Step 1: Load the dataset properly with quotation handling
df = pd.read_csv('/content/Dataset .csv', sep=';', quotechar='"', skipinitialspace=True)

# Step 2: Split the column only once (assuming the entire data is in the first column)
df = df.iloc[:, 0].str.split(',', expand=True)

# Step 3: Check the number of columns
print(f"Number of columns after splitting: {df.shape[1]}")

# Step 4: Assign the correct column names (36 columns)
column_names = [
    'Restaurant ID', 'Restaurant Name', 'Country Code', 'City', 'Address',
    'Locality', 'Locality Verbose', 'Longitude', 'Latitude', 'Cuisines',
    'Average Cost for two', 'Currency', 'Has Table booking', 'Has Online delivery',
    'Is delivering now', 'Switch to order menu', 'Price range', 'Aggregate rating',
    'Rating color', 'Rating text', 'Votes', 'Extra_1', 'Extra_2', 'Extra_3',
    'Extra_4', 'Extra_5', 'Extra_6', 'Extra_7', 'Extra_8', 'Extra_9',
    'Extra_10', 'Extra_11', 'Extra_12', 'Extra_13', 'Extra_14', 'Extra_15'
]

# Step 5: Apply column names (ensure the length matches)
df.columns = column_names

# Step 6: Drop the unnecessary extra columns if they contain only null values
df = df.loc[:, ~df.columns.str.startswith('Extra_') | df.notna().any()]

# Step 7: Verify the cleaned DataFrame
print("Final DataFrame shape:", df.shape)
print(df.head())

import pandas as pd

# Reload the dataset using the correct delimiter
try:
    df = pd.read_csv('Dataset .csv', delimiter=',')
    display(df.head())
except FileNotFoundError:
    print("Error: 'Dataset .csv' not found.")
    df = pd.DataFrame() # Create an empty DataFrame to avoid errors later
except pd.errors.ParserError:
    print("Error: Could not parse the file. Please check the delimiter or file format.")
    df = pd.DataFrame() # Create an empty DataFrame to avoid errors later

# Identify the target variable
target_variable = 'Aggregate rating'
print(f"\nTarget Variable: {target_variable}")

# Determine data types and check for inconsistencies
print("\nData Types:")
print(df.dtypes)
print("\nShape of DataFrame:")
print(df.shape)

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Analyze the distribution of the target variable
print("\nDistribution of Target Variable:")
print(df[target_variable].describe())

# Examine the distributions of other relevant features
print("\nDistribution of other relevant features:")
for col in ['Average Cost for two', 'Price range', 'Votes']:
    print(f"\n{col}:")
    print(df[col].describe())
    print(df[col].value_counts())

"""## Data cleaning

### Subtask:
Clean the data by handling missing values and removing duplicate rows.

**Reasoning**:
Handle missing values in the 'Cuisines' column by removing rows with missing values and remove duplicate rows, then reset the index.
"""

# Drop rows with missing 'Cuisines' values
df.dropna(subset=['Cuisines'], inplace=True)

# Remove duplicate rows and reset the index
df.drop_duplicates(inplace=True)
df.reset_index(drop=True, inplace=True)

# Display some info
print(df.shape)
print(df.isnull().sum())

"""## Data wrangling

### Subtask:
Encode categorical features and convert numerical features to appropriate data types.

**Reasoning**:
Encode categorical features using one-hot encoding and convert numerical features to appropriate data types. Handle potential errors during type conversion gracefully.
"""

import pandas as pd

# Step 1: Load the dataset properly (comma-separated, with quotes handling)
df = pd.read_csv('/content/Dataset .csv', sep=',', quotechar='"', skipinitialspace=True)

# Step 2: Inspect the data
print("Initial Data Shape:", df.shape)
print("Column Names:", df.columns)
print(df.head())

# Step 3: Identify categorical and numerical columns
numerical_cols = ['Average Cost for two', 'Price range', 'Votes', 'Aggregate rating']
categorical_cols = [col for col in df.columns if col not in numerical_cols]

print("\nCategorical Columns:", categorical_cols)
print("Numerical Columns:", numerical_cols)

# Step 4: Handle missing values in categorical columns
df[categorical_cols] = df[categorical_cols].fillna('Unknown')

# Step 5: One-hot encode categorical columns
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Step 6: Convert numerical columns to numeric (handling errors)
for col in numerical_cols:
    df_encoded[col] = pd.to_numeric(df_encoded[col], errors='coerce')  # Convert invalid to NaN
    df_encoded[col].fillna(df_encoded[col].mean(), inplace=True)  # Fill NaNs with the mean

# Step 7: Check the updated DataFrame
print("\nProcessed Data Shape:", df_encoded.shape)
print(df_encoded.head())
print(df_encoded.dtypes)

# Step 8: Analyze the target variable
print("\nDistribution of Aggregate Rating:")
print(df_encoded['Aggregate rating'].describe())

# Step 9: Examine distributions of numerical features
for col in numerical_cols:
    print(f"\n{col} Distribution:")
    print(df_encoded[col].describe())

# Identify categorical columns (excluding the target variable)
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
if 'Aggregate rating' in categorical_cols:
    categorical_cols.remove('Aggregate rating')

# One-hot encode categorical features
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Identify numerical columns
numerical_cols = ['Average Cost for two', 'Price range', 'Votes']

# Convert numerical columns to appropriate data types and handle errors
for col in numerical_cols:
    try:
        # Attempt conversion to numeric, coercing errors to NaN
        df_encoded[col] = pd.to_numeric(df_encoded[col], errors='coerce')

        # Convert to appropriate integer type, handling potential errors
        if col in ['Price range', 'Votes']:
            df_encoded[col] = df_encoded[col].astype('Int64')
        else:
            df_encoded[col] = df_encoded[col].astype('Int64')
    except (ValueError, TypeError) as e:
        print(f"Error converting column '{col}': {e}")
        # More robust error handling: fill NaN values with the median
        df_encoded[col] = pd.to_numeric(df_encoded[col], errors='coerce')
        df_encoded[col] = df_encoded[col].fillna(df_encoded[col].median())
        df_encoded[col] = df_encoded[col].astype('Int64')

display(df_encoded.head())
print(df_encoded.dtypes)

"""## Feature engineering

### Subtask:
Engineer new features from existing ones in the `df_encoded` DataFrame to potentially improve model performance.

**Reasoning**:
Create interaction terms and ratios between relevant features in `df_encoded` to potentially improve model performance.  Then, evaluate the new features, and remove features that do not improve the model's performance.
"""

# Create interaction terms
df_encoded['Cost_x_Price'] = df_encoded['Average Cost for two'] * df_encoded['Price range']
df_encoded['Votes_div_Cost'] = df_encoded['Votes'] / df_encoded['Average Cost for two']

# Evaluate new features and remove if necessary (placeholder, needs further analysis and model evaluation)
# This is a placeholder, in a real-world scenario, model evaluation metrics such as R-squared or RMSE would be used
# Here, I'm simply checking for potential issues like infinite values or NaNs

# Handle potential infinite values from division by zero
df_encoded['Votes_div_Cost'].replace([float('inf'), float('-inf')], 0, inplace=True)

# Fill any NaN values with the median
df_encoded['Votes_div_Cost'].fillna(df_encoded['Votes_div_Cost'].median(), inplace=True)
df_encoded['Cost_x_Price'].fillna(df_encoded['Cost_x_Price'].median(), inplace=True)


display(df_encoded.head())

"""**Reasoning**:
The previous code block had some warnings about chained assignment.  I will fix those warnings and then finish the task.


"""

# Create interaction terms
df_encoded['Cost_x_Price'] = df_encoded['Average Cost for two'] * df_encoded['Price range']
df_encoded['Votes_div_Cost'] = df_encoded['Votes'] / df_encoded['Average Cost for two']

# Handle potential infinite values from division by zero
df_encoded['Votes_div_Cost'] = df_encoded['Votes_div_Cost'].replace([float('inf'), float('-inf')], 0)

# Fill any NaN values with the median
df_encoded['Votes_div_Cost'] = df_encoded['Votes_div_Cost'].fillna(df_encoded['Votes_div_Cost'].median())
df_encoded['Cost_x_Price'] = df_encoded['Cost_x_Price'].fillna(df_encoded['Cost_x_Price'].median())

display(df_encoded.head())

"""## Data splitting

### Subtask:
Split the data into training, testing, and validation sets.

**Reasoning**:
Split the data into training, testing, and validation sets as instructed.
"""

from sklearn.model_selection import train_test_split

# Separate features (X) and target variable (y)
X = df_encoded.drop('Aggregate rating', axis=1)
y = df_encoded['Aggregate rating']

# Split data into training and temporary sets (no stratification)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Split temporary set into testing and validation sets (no stratification)
X_test, X_val, y_test, y_val = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

print("Train Size:", X_train.shape, y_train.shape)
print("Test Size:", X_test.shape, y_test.shape)
print("Validation Size:", X_val.shape, y_val.shape)

from sklearn.model_selection import train_test_split

# Step 1: Identify classes with fewer than 2 samples to avoid stratification issues
class_counts = df_encoded['Aggregate rating'].value_counts()
classes_to_keep = class_counts[class_counts >= 2].index

# Step 2: Filter dataset to keep only classes with enough samples
df_filtered = df_encoded[df_encoded['Aggregate rating'].isin(classes_to_keep)]

# Step 3: Separate features (X) and target (y)
X = df_filtered.drop('Aggregate rating', axis=1)
y = df_filtered['Aggregate rating']

# Step 4: Initial train-test split (80% train, 20% temp) with stratification if possible
if len(y.unique()) > 1:
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
else:
    print("Warning: Not enough unique classes for stratification in the initial split. Using random split.")
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

# Step 5: Check class counts in y_temp and handle low-sample classes
temp_class_counts = y_temp.value_counts()
classes_to_keep_temp = temp_class_counts[temp_class_counts >= 2].index

# Step 6: Filter temporary set to remove classes with fewer than 2 samples
X_temp_filtered = X_temp[y_temp.isin(classes_to_keep_temp)]
y_temp_filtered = y_temp[y_temp.isin(classes_to_keep_temp)]

# Step 7: Second split - 10% Test, 10% Validation, stratify if possible
if len(y_temp_filtered.unique()) > 1:
    X_test, X_val, y_test, y_val = train_test_split(
        X_temp_filtered, y_temp_filtered, test_size=0.5, random_state=42, stratify=y_temp_filtered
    )
else:
    print("Warning: Not enough unique classes for stratification in the temp split. Using random split.")
    X_test, X_val, y_test, y_val = train_test_split(
        X_temp_filtered, y_temp_filtered, test_size=0.5, random_state=42
    )

# Step 8: Print the shape of each split
print("Training Set Shape:", X_train.shape, y_train.shape)
print("Validation Set Shape:", X_val.shape, y_val.shape)
print("Test Set Shape:", X_test.shape, y_test.shape)

"""**Reasoning**:
The error message `TypeError: '<' not supported between instances of 'str' and 'float'` indicates that the `stratify` parameter in `train_test_split` is receiving a mix of string and float values.  This is likely due to the 'other' class introduced in the previous attempt. I will explicitly convert the target variable `y` to string type before the split to resolve this.


"""

import numpy as np
from sklearn.model_selection import train_test_split

# Identify classes with only one member
class_counts = df_encoded['Aggregate rating'].value_counts()
classes_to_combine = class_counts[class_counts < 2].index

# Combine the least populated classes into a single class
df_encoded['Aggregate rating'] = df_encoded['Aggregate rating'].replace(classes_to_combine, 'other')

# Separate features (X) and target variable (y)
X = df_encoded.drop('Aggregate rating', axis=1)
y = df_encoded['Aggregate rating'].astype(str)  # Convert y to string type

# Split data into training and temporary sets
# Instead of stratifying, use a simple random split to avoid the error
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42
    # Remove stratify=y to perform a random split
)

# Split temporary set into testing and validation sets
# Instead of stratifying, use a simple random split to avoid the error
X_test, X_val, y_test, y_val = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
    # Remove stratify=y_temp to perform a random split
)

"""## Model training

### Subtask:
Train a linear regression model on the training data (X_train, y_train) and evaluate its performance on the validation set (X_val, y_val).  Consider alternative models if the initial model's performance is not satisfactory.

**Reasoning**:
Train a linear regression model and evaluate its performance. Then, explore alternative models if necessary.
"""

# Remove 'other' from the target, as it's non-numeric and cannot be used in regression
mask = y != 'other'
X_train, X_val, y_train, y_val = X_train[mask], X_val[mask], y_train[mask], y_val[mask]

# Convert y back to float
y_train = y_train.astype(float)
y_val = y_val.astype(float)

# Import models and metrics
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Initialize and train a linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
linear_predictions = linear_model.predict(X_val)
linear_mse = mean_squared_error(y_val, linear_predictions)
linear_r2 = r2_score(y_val, linear_predictions)
print(f"Linear Regression - MSE: {linear_mse}, R-squared: {linear_r2}")

# Initialize and train a Decision Tree Regressor
tree_model = DecisionTreeRegressor(random_state=42)
tree_model.fit(X_train, y_train)
tree_predictions = tree_model.predict(X_val)
tree_mse = mean_squared_error(y_val, tree_predictions)
tree_r2 = r2_score(y_val, tree_predictions)
print(f"Decision Tree Regression - MSE: {tree_mse}, R-squared: {tree_r2}")

# Initialize and train a Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_val)
rf_mse = mean_squared_error(y_val, rf_predictions)
rf_r2 = r2_score(y_val, rf_predictions)
print(f"Random Forest Regression - MSE: {rf_mse}, R-squared: {rf_r2}")

import numpy as np

# Identify indices of rows with 'other' in y_train
other_indices_train = np.where(y_train == 'other')[0]

# Remove those rows
X_train = X_train.drop(index=other_indices_train)
y_train = y_train.drop(index=other_indices_train)

# Identify indices of rows with 'other' in y_val
other_indices_val = np.where(y_val == 'other')[0]

# Remove those rows
X_val = X_val.drop(index=other_indices_val)
y_val = y_val.drop(index=other_indices_val)

# Reset the indices
X_train = X_train.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
X_val = X_val.reset_index(drop=True)
y_val = y_val.reset_index(drop=True)

"""**Reasoning**:
The error persists, indicating that the 'other' values are still present in `y_train` despite the previous code.  I will inspect the `y_train` and `y_val` to understand why and then remove those rows again.


"""

print(y_train.value_counts())
print(y_val.value_counts())

"""**Reasoning**:
The value counts show that there is still one 'other' value in `y_train`. I will repeat the data cleaning step to remove this value and then re-run the model training.


"""

import numpy as np

# Identify indices of rows with 'other' in y_train
other_indices_train = np.where(y_train == 'other')[0]

# Remove those rows
X_train = X_train.drop(index=other_indices_train)
y_train = y_train.drop(index=other_indices_train)

# Reset the indices
X_train = X_train.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Initialize and train a linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train.astype(float))
linear_predictions = linear_model.predict(X_val)
linear_mse = mean_squared_error(y_val.astype(float), linear_predictions)
linear_r2 = r2_score(y_val.astype(float), linear_predictions)
print(f"Linear Regression - MSE: {linear_mse}, R-squared: {linear_r2}")

# Initialize and train a Decision Tree Regressor
tree_model = DecisionTreeRegressor(random_state=42)
tree_model.fit(X_train, y_train.astype(float))
tree_predictions = tree_model.predict(X_val)
tree_mse = mean_squared_error(y_val.astype(float), tree_predictions)
tree_r2 = r2_score(y_val.astype(float), tree_predictions)
print(f"Decision Tree Regression - MSE: {tree_mse}, R-squared: {tree_r2}")

# Initialize and train a Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train.astype(float))
rf_predictions = rf_model.predict(X_val)
rf_mse = mean_squared_error(y_val.astype(float), rf_predictions)
rf_r2 = r2_score(y_val.astype(float), rf_predictions)
print(f"Random Forest Regression - MSE: {rf_mse}, R-squared: {rf_r2}")

"""## Model evaluation

### Subtask:
Evaluate the performance of the trained Random Forest model on the test set (X_test, y_test).

**Reasoning**:
Evaluate the trained Random Forest model on the test set and calculate the evaluation metrics.
"""

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

# Predict on the test set
rf_predictions = rf_model.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test.astype(float), rf_predictions)
rmse = np.sqrt(mse)
r2 = r2_score(y_test.astype(float), rf_predictions)
mae = mean_absolute_error(y_test.astype(float), rf_predictions)

# Print the evaluation metrics
print(f"Random Forest Regression - Test Set Performance:")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")
print(f"Mean Absolute Error (MAE): {mae}")

"""## Data visualization

### Subtask:
Visualize the model's performance and feature importances.

**Reasoning**:
Visualize the model's performance and feature importances using residual plot, bar chart, and scatter plot.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Residual plot
plt.figure(figsize=(8, 6))
residuals = y_test.astype(float) - rf_predictions
sns.residplot(x=rf_predictions, y=residuals, lowess=True, color="g")
plt.xlabel("Predicted Aggregate Rating")
plt.ylabel("Residuals")
plt.title("Residual Plot of Random Forest Model")
plt.show()

# Feature Importance
feature_importances = rf_model.feature_importances_
top_10_features = np.argsort(feature_importances)[::-1][:10]
plt.figure(figsize=(10, 6))
plt.barh(X_test.columns[top_10_features], feature_importances[top_10_features], color='skyblue')
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Top 10 Feature Importances from Random Forest Model")
plt.show()

# Scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(y_test.astype(float), rf_predictions, color='purple', alpha=0.6)
plt.plot([min(y_test.astype(float)), max(y_test.astype(float))], [min(y_test.astype(float)), max(y_test.astype(float))], color='red', linestyle='--')
plt.xlabel("Actual Aggregate Rating")
plt.ylabel("Predicted Aggregate Rating")
plt.title("Actual vs. Predicted Aggregate Ratings")
plt.show()

# prompt: provide linear regression graph for this project

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'linear_model', 'X_val', and 'y_val' are defined from your previous code

# Predict using the linear regression model
linear_predictions = linear_model.predict(X_val)

# Create the scatter plot
plt.figure(figsize=(8, 6))
sns.regplot(x=y_val.astype(float), y=linear_predictions, ci=None, color='blue')  # Use regplot for linear regression line
plt.xlabel("Actual Aggregate Rating")
plt.ylabel("Predicted Aggregate Rating")
plt.title("Linear Regression: Actual vs. Predicted Aggregate Ratings")
plt.show()

"""## Summary:

### 1. Q&A

* **What is the best performing model for predicting restaurant aggregate ratings?**  The Random Forest Regressor demonstrated the best performance, achieving an R-squared of 0.9860 on the validation set and 0.989 on the test set.

* **What are the most influential features affecting restaurant ratings?**  The code identifies the top 10 most important features based on the Random Forest model's feature importance scores.  The specific features are not listed in the summary but are visualized in a bar chart in the "Data Visualization" step.

### 2. Data Analysis Key Findings

* **Data Cleaning:**  Removed 9 rows with missing 'Cuisines' values and duplicate rows, resulting in a final dataset with 9542 rows.
* **Feature Engineering:** Created interaction terms (`Cost_x_Price`, `Votes_div_Cost`) to potentially improve model performance.  Handled potential infinite values from division by zero.
* **Model Performance (Validation Set):**  Random Forest Regression achieved the best results with MSE: 0.0304 and R-squared: 0.9860
* **Model Performance (Test Set):**  Random Forest Regression achieved MSE: 0.0267, RMSE: 0.163, R-squared: 0.989, and MAE: 0.105.

### 3. Insights or Next Steps

* **Analyze Feature Importance:**  Deep dive into the top 10 features to understand their specific influence on ratings.  This could reveal actionable insights for restaurants.
* **Hyperparameter Tuning:** Further tune the hyperparameters of the Random Forest model (e.g., number of trees, tree depth, minimum samples per leaf) to potentially improve performance.

"""



"""## This project Done by Manikandan M

"""